{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "tyXXnW0WbOnw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "np.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "NSB-N1MDbYux",
        "outputId": "46eb9a8f-4428-4f9e-eff9-8ef5da7abfe7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.25.2'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(42) # reproducibility"
      ],
      "metadata": {
        "id": "YWwBVXfmrSiu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction to Gradient Descent"
      ],
      "metadata": {
        "id": "0rjF8Q4dbesX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gradient descent is a fundamental optimization algorithm used in machine learning and optimization problems to minimize the cost function or loss function. The concept revolves around iteratively adjusting the parameters of a model in the direction of the steepest descent of the gradient of the cost function. The gradient represents the direction of the steepest increase in the function. In each iteration, the algorithm calculates the gradient of the cost function with respect to the parameters, and then updates the parameters in the opposite direction of the gradient by a certain step size known as the learning rate. This process continues until convergence, where the gradient becomes nearly zero, indicating that the algorithm has reached a local minimum."
      ],
      "metadata": {
        "id": "9ZN3i8GWbh9a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prerequisites"
      ],
      "metadata": {
        "id": "Gi57LICHL1Q_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will take a sigmoid neuron to show the process of gradient descent algorithms and it's different variations (with optimization techniques)"
      ],
      "metadata": {
        "id": "tVrsG6DwI3Ho"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Function to Replicate"
      ],
      "metadata": {
        "id": "WTQFTAhrZxVB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The Sigmoid Function**\n",
        "\n",
        "The sigmoid function is a mathematical function that maps any real-valued number to a value between 0 and 1. It introduces non-linearity to the network and enables it to learn complex patterns in the data.\n",
        "\n",
        "**Properties**\n",
        "\n",
        "- **Range**: The output of the sigmoid function is always between 0 and 1. As \\( x \\) approaches negative infinity, the output approaches 0, and as \\( x \\) approaches positive infinity, the output approaches 1.\n",
        "- **Smoothness**: The sigmoid function is smooth and differentiable everywhere, which makes it suitable for optimization algorithms such as gradient descent.\n",
        "- **S-shaped curve**: The graph of the sigmoid function resembles the letter \"S\", hence the name \"sigmoid\". This shape introduces non-linearity to the network, allowing it to model complex relationships in the data.\n"
      ],
      "metadata": {
        "id": "xU5GFLFthuUF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(W: np.array, b: np.array, X: np.array):\n",
        "    return (1 / (1 + np.exp(-(np.dot(X, W) + b))))"
      ],
      "metadata": {
        "id": "JfamfYVvMLjx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instead of the formula and array sizes of np.dot.. we can use something like [this](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html)\n",
        "\n"
      ],
      "metadata": {
        "id": "CsUmAUX-8TR7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loss Function"
      ],
      "metadata": {
        "id": "nCEWuet7Z1y_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mean squared error (MSE) is a fundamental metric used in statistics and machine learning to quantify the average squared difference between the actual values and the predicted values. It is calculated by taking the average of the squared differences between the predicted and true values for each data point.\n",
        "\n",
        "MSE is favored for its mathematical properties, such as being non-negative and sensitive to the magnitude of errors, making it a widely adopted measure for evaluating model performance and guiding optimization efforts."
      ],
      "metadata": {
        "id": "bBMDORLtZ6Ji"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def error(W: np.array, b: np.array, X: np.array, y_true: np.array):\n",
        "    m = X.shape[0]\n",
        "\n",
        "    y_hat = sigmoid(W, b, X)\n",
        "\n",
        "    cost = np.sum((y_hat - y_true)**2, axis = 0) / m\n",
        "\n",
        "    return cost"
      ],
      "metadata": {
        "id": "5pLerj-YZ5qM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generating the Data"
      ],
      "metadata": {
        "id": "tGpD1pgpLvui"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Taking number of features as 2 and instances as 100\n",
        "n_features = 2\n",
        "m = 100"
      ],
      "metadata": {
        "id": "mCJkrsX9MMyw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "W_true = np.full((n_features, 1), 0.5)\n",
        "b_true = 1\n",
        "\n",
        "# We are only focused on the algorithm, so we generate this randomly\n",
        "\n",
        "X = np.random.rand(m, n_features)\n",
        "\n",
        "y_true = sigmoid(W_true, b_true, X)\n",
        "y_true.shape, X.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8b1gOtlQYLv8",
        "outputId": "d8b10c6f-0012-4620-b9ae-74c37a2ccd05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((100, 1), (100, 2))"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Calculating Gradients"
      ],
      "metadata": {
        "id": "zQzK0hdNsDem"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def grad(W: np.array, b: np.array, X: np.array, y_true: np.array):\n",
        "    n_features = X.shape[0]\n",
        "\n",
        "    # Calculating predicted y\n",
        "    y_hat = sigmoid(W, b, X)\n",
        "    grad_W_i = np.zeros((n_features, 1))\n",
        "\n",
        "    for j in range(n_features):\n",
        "        grad_W_i[j] = np.dot((y_hat - y_true) * (y_hat) * (1 - y_hat), X[j])\n",
        "    grad_b_i = (y_hat - y_true) * (y_hat) * (1 - y_hat)\n",
        "\n",
        "    return grad_W_i, grad_b_i"
      ],
      "metadata": {
        "id": "TZ6DVtyGML5b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initializing Weights & Biases"
      ],
      "metadata": {
        "id": "VmYB1FJTtOfN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def init_weights(n_features):\n",
        "    W = np.random.rand(n_features, 1)\n",
        "    b = 1\n",
        "    return W, b"
      ],
      "metadata": {
        "id": "qBEejaOXtSBB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vanilla (Batch) Gradient Descent"
      ],
      "metadata": {
        "id": "ITRihKxktisa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def batch_gradient_descent(X: np.array, y_true: np.array, epochs = 100, learning_rate = 0.1):\n",
        "    m = X.shape[0]\n",
        "    W, b = init_weights(n_features)\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        grad_W = np.zeros_like(W)\n",
        "        grad_b = 0\n",
        "\n",
        "        # Compute gradients\n",
        "        for i in range(m):\n",
        "            grad_W_i, grad_b_i = grad(W, b, X[i], y_true[i])\n",
        "            grad_W += grad_W_i\n",
        "            grad_b += grad_b_i\n",
        "\n",
        "\n",
        "        # Update weights and bias\n",
        "        W -= learning_rate * grad_W\n",
        "        b -= learning_rate * grad_b\n",
        "\n",
        "        # Printing the Progress\n",
        "        if epoch % 20 == 0:\n",
        "            print(f\"Epoch: {epoch}\")\n",
        "            print(f\"Weights: {W}\")\n",
        "            print(f\"Bias: {b}\")\n",
        "            print(\"-----------------------------------\")\n",
        "\n",
        "    return W, b"
      ],
      "metadata": {
        "id": "qXX0UNZRtrCC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_gradient_descent(X, y_true, epochs = 100, learning_rate = 0.03)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o1Djn5_QwAx7",
        "outputId": "88172fb7-31c5-4967-bcfa-ee681f43e817"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 20\n",
            "Weights: [[0.65266844]\n",
            " [0.16722706]]\n",
            "Bias: [1.08095925]\n",
            "-----------------------------------\n",
            "Epoch: 40\n",
            "Weights: [[0.63362175]\n",
            " [0.20515667]]\n",
            "Bias: [1.08376961]\n",
            "-----------------------------------\n",
            "Epoch: 60\n",
            "Weights: [[0.61403602]\n",
            " [0.23517087]]\n",
            "Bias: [1.07853052]\n",
            "-----------------------------------\n",
            "Epoch: 80\n",
            "Weights: [[0.59671899]\n",
            " [0.26157269]]\n",
            "Bias: [1.07278195]\n",
            "-----------------------------------\n",
            "Epoch: 100\n",
            "Weights: [[0.58172374]\n",
            " [0.28517517]]\n",
            "Bias: [1.06741332]\n",
            "-----------------------------------\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[0.58172374],\n",
              "        [0.28517517]]),\n",
              " array([1.06741332]))"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stochastic Gradient Descent"
      ],
      "metadata": {
        "id": "kQxD3c4M1MfK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   Here, the parameters are updates for every data point passed through in the epoch. (Number of updates per epoch are number of instances in the data)\n",
        "*   It is an approximate (rather stochastic) gradient.\n",
        "*   No guarantee that the loss decreases every step.\n",
        "\n"
      ],
      "metadata": {
        "id": "chcTcjku1QQ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def stochastic_gradient_descent(X: np.array, y_true: np.array, epochs = 100, learning_rate = 0.1):\n",
        "    m = X.shape[0]\n",
        "    W, b = init_weights(n_features)\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        grad_W = np.zeros_like(W)\n",
        "        grad_b = 0\n",
        "\n",
        "        # Compute gradients\n",
        "        for i in range(m):\n",
        "            grad_W_i, grad_b_i = grad(W, b, X[i], y_true[i])\n",
        "            grad_W += grad_W_i\n",
        "            grad_b += grad_b_i\n",
        "\n",
        "            # Update weights and bias\n",
        "            W -= learning_rate * grad_W\n",
        "            b -= learning_rate * grad_b\n",
        "\n",
        "        # Printing the Progress\n",
        "        if epoch % 10 == 0:\n",
        "            print(f\"Epoch: {epoch}\")\n",
        "            print(f\"Weights: {W}\")\n",
        "            print(f\"Bias: {b}\")\n",
        "            print(\"-----------------------------------\")\n",
        "\n",
        "    return W, b"
      ],
      "metadata": {
        "id": "K60iG6W61L1D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stochastic_gradient_descent(X, y_true, epochs = 100, learning_rate = 0.03)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ic4NRvZS2eW3",
        "outputId": "08697a08-eaf2-4a24-d32f-c04518f678a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 10\n",
            "Weights: [[0.49435003]\n",
            " [0.51465431]]\n",
            "Bias: [0.99881913]\n",
            "-----------------------------------\n",
            "Epoch: 20\n",
            "Weights: [[0.50099308]\n",
            " [0.50182244]]\n",
            "Bias: [1.00167229]\n",
            "-----------------------------------\n",
            "Epoch: 30\n",
            "Weights: [[0.50083917]\n",
            " [0.5011882 ]]\n",
            "Bias: [1.00209647]\n",
            "-----------------------------------\n",
            "Epoch: 40\n",
            "Weights: [[0.50080871]\n",
            " [0.50115223]]\n",
            "Bias: [1.00219546]\n",
            "-----------------------------------\n",
            "Epoch: 50\n",
            "Weights: [[0.50082   ]\n",
            " [0.50117094]]\n",
            "Bias: [1.00225238]\n",
            "-----------------------------------\n",
            "Epoch: 60\n",
            "Weights: [[0.50083778]\n",
            " [0.50119679]]\n",
            "Bias: [1.00230473]\n",
            "-----------------------------------\n",
            "Epoch: 70\n",
            "Weights: [[0.50085682]\n",
            " [0.50122411]]\n",
            "Bias: [1.00235751]\n",
            "-----------------------------------\n",
            "Epoch: 80\n",
            "Weights: [[0.5008764 ]\n",
            " [0.50125217]]\n",
            "Bias: [1.00241139]\n",
            "-----------------------------------\n",
            "Epoch: 90\n",
            "Weights: [[0.50089645]\n",
            " [0.5012809 ]]\n",
            "Bias: [1.00246648]\n",
            "-----------------------------------\n",
            "Epoch: 100\n",
            "Weights: [[0.50091695]\n",
            " [0.50131028]]\n",
            "Bias: [1.00252284]\n",
            "-----------------------------------\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[0.50091695],\n",
              "        [0.50131028]]),\n",
              " array([1.00252284]))"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mini-Batch Gradient Descent"
      ],
      "metadata": {
        "id": "7S3aGNb02njG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   Instead of updating parameters for every data points we only pick a small batch of data points to\n",
        "*   Better estimate of the 'true' gradient\n",
        "\n"
      ],
      "metadata": {
        "id": "RhhzVfR-2veH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 10"
      ],
      "metadata": {
        "id": "N-2VRHTh2rIM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def minibatch_gradient_descent(X: np.array, y_true: np.array, epochs = 100, learning_rate = 0.1):\n",
        "    m = X.shape[0]\n",
        "    W, b = init_weights(n_features)\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        grad_W = np.zeros_like(W)\n",
        "        grad_b = 0\n",
        "\n",
        "        # Compute gradients\n",
        "        for i in range(1, m):\n",
        "            grad_W_i, grad_b_i = grad(W, b, X[i], y_true[i])\n",
        "            grad_W += grad_W_i\n",
        "            grad_b += grad_b_i\n",
        "\n",
        "            if i % BATCH_SIZE == 0:\n",
        "            # Update weights and bias\n",
        "                W -= learning_rate * grad_W\n",
        "                b -= learning_rate * grad_b\n",
        "\n",
        "        # Printing the Progress\n",
        "        if epoch % 10 == 0:\n",
        "            print(f\"Epoch: {epoch}\")\n",
        "            print(f\"Weights: {W}\")\n",
        "            print(f\"Bias: {b}\")\n",
        "            print(\"-----------------------------------\")\n",
        "\n",
        "    return W, b"
      ],
      "metadata": {
        "id": "_aIYDmMQ3Iaj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "minibatch_gradient_descent(X, y_true, epochs = 100, learning_rate = 0.03)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wyqIXYEa3Qof",
        "outputId": "c61f1e54-a905-4507-9254-3e7186b03219"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 10\n",
            "Weights: [[0.60500726]\n",
            " [0.17472198]]\n",
            "Bias: [1.11611545]\n",
            "-----------------------------------\n",
            "Epoch: 20\n",
            "Weights: [[0.56581798]\n",
            " [0.24459342]]\n",
            "Bias: [1.09671509]\n",
            "-----------------------------------\n",
            "Epoch: 30\n",
            "Weights: [[0.53876492]\n",
            " [0.29855737]]\n",
            "Bias: [1.0806513]\n",
            "-----------------------------------\n",
            "Epoch: 40\n",
            "Weights: [[0.52042415]\n",
            " [0.34050999]]\n",
            "Bias: [1.06752084]\n",
            "-----------------------------------\n",
            "Epoch: 50\n",
            "Weights: [[0.50820345]\n",
            " [0.37323379]]\n",
            "Bias: [1.05667633]\n",
            "-----------------------------------\n",
            "Epoch: 60\n",
            "Weights: [[0.5002627 ]\n",
            " [0.39885386]]\n",
            "Bias: [1.04765572]\n",
            "-----------------------------------\n",
            "Epoch: 70\n",
            "Weights: [[0.49529699]\n",
            " [0.4189903 ]]\n",
            "Bias: [1.04011641]\n",
            "-----------------------------------\n",
            "Epoch: 80\n",
            "Weights: [[0.49238124]\n",
            " [0.43487914]]\n",
            "Bias: [1.03379519]\n",
            "-----------------------------------\n",
            "Epoch: 90\n",
            "Weights: [[0.49086039]\n",
            " [0.44746561]]\n",
            "Bias: [1.02848414]\n",
            "-----------------------------------\n",
            "Epoch: 100\n",
            "Weights: [[0.49027173]\n",
            " [0.45747475]]\n",
            "Bias: [1.02401564]\n",
            "-----------------------------------\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[0.49027173],\n",
              "        [0.45747475]]),\n",
              " array([1.02401564]))"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introducing Momentum in Gradient Descent"
      ],
      "metadata": {
        "id": "haOCxtyh3vTS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imagine you're riding a bike down a bumpy hill.\n",
        "\n",
        "With regular gradient descent, you'd pedal harder when the hill gets steeper and ease off when it levels out. But this can make your ride jerky and slow.\n",
        "\n",
        "Now, with momentum, you pedal not only based on what you see right in front of you but also on how fast you were going before. So, even if the hill gets a bit bumpy, you keep moving forward more steadily because you're carrying some of your previous speed with you. It's like having a little push from your past self, making your ride smoother and faster overall."
      ],
      "metadata": {
        "id": "S7lgVnmE363l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Momentum-Based Gradient Descent"
      ],
      "metadata": {
        "id": "rd_nwVri4X7n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This enhances traditional gradient descent by incorporating a momentum term. This momentum term allows the algorithm to build inertia in a specific direction during the search process, aiding in faster convergence and overcoming issues like local minima and oscillations.\n",
        "\n",
        "By adding a fraction of the previous weight update to the current update, momentum-based gradient descent accelerates optimization by smoothing out weight updates, reducing model complexity, and preventing overfitting.\n",
        "\n",
        "The momentum term is calculated as a moving average of past gradients, with the weight of these past gradients controlled by a hyperparameter called Momentum Constant.\n",
        "\n",
        "This technique helps the algorithm escape local minima and saddle points, converge faster, and stabilize the optimization process"
      ],
      "metadata": {
        "id": "0iwD6Hn16vhY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def momentum_based_gradient_descent(X: np.array, y_true: np.array, epochs = 100, momentum_constant = 0.9, learning_rate = 0.1):\n",
        "    m = X.shape[0]\n",
        "    W, b = init_weights(n_features)\n",
        "    update_W, update_b = np.zeros_like(W), 0\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        grad_W = np.zeros_like(W)\n",
        "        grad_b = 0\n",
        "\n",
        "        # Compute gradients\n",
        "        for i in range(m):\n",
        "            grad_W_i, grad_b_i = grad(W, b, X[i], y_true[i])\n",
        "            grad_W += grad_W_i\n",
        "            grad_b += grad_b_i\n",
        "\n",
        "        # Dealing with the history\n",
        "        update_W = momentum_constant * update_W + learning_rate * grad_W\n",
        "        update_b = momentum_constant * update_b + learning_rate * grad_b\n",
        "\n",
        "        # Update weights and bias\n",
        "        W -= update_W\n",
        "        b -= update_b\n",
        "\n",
        "        # Printing the Progress\n",
        "        if epoch % 20 == 0:\n",
        "            print(f\"Epoch: {epoch}\")\n",
        "            print(f\"Weights: {W}\")\n",
        "            print(f\"Bias: {b}\")\n",
        "            print(\"-----------------------------------\")\n",
        "\n",
        "    return W, b"
      ],
      "metadata": {
        "id": "qK8ON-xI3yav"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "momentum_based_gradient_descent(X, y_true, epochs = 100, momentum_constant = 0.9, learning_rate = 0.03)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yPUDY6Ai4xo4",
        "outputId": "ba9c1e70-cd76-49e8-9001-80c659c04c80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 20\n",
            "Weights: [[0.33281891]\n",
            " [0.55531903]]\n",
            "Bias: [1.00928586]\n",
            "-----------------------------------\n",
            "Epoch: 40\n",
            "Weights: [[0.49232534]\n",
            " [0.47910667]]\n",
            "Bias: [0.99807405]\n",
            "-----------------------------------\n",
            "Epoch: 60\n",
            "Weights: [[0.51372637]\n",
            " [0.48298288]]\n",
            "Bias: [0.99648827]\n",
            "-----------------------------------\n",
            "Epoch: 80\n",
            "Weights: [[0.50314529]\n",
            " [0.49727586]]\n",
            "Bias: [0.99803748]\n",
            "-----------------------------------\n",
            "Epoch: 100\n",
            "Weights: [[0.49897413]\n",
            " [0.50132876]]\n",
            "Bias: [0.99923048]\n",
            "-----------------------------------\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[0.49897413],\n",
              "        [0.50132876]]),\n",
              " array([0.99923048]))"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Nesterov Accelerated Gradient Descent"
      ],
      "metadata": {
        "id": "E25wPie27Tx7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "While the idea is similar to Momentum-Based Gradient Descent, here we look ahead if we are overshooting the (local) minima and move accordingly.\n",
        "\n",
        "This technique helps correct the course of the gradient descent quicker and hence the oscillations are less"
      ],
      "metadata": {
        "id": "CQGbpJli7a-F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def nesterov_accelerated_gradient_descent(X: np.array, y_true: np.array, epochs = 100, momentum_constant = 0.9, learning_rate = 0.1):\n",
        "    m = X.shape[0]\n",
        "    W, b = init_weights(n_features)\n",
        "    update_W, update_b = np.zeros_like(W), 0\n",
        "    lookahead_W, lookahead_b = np.zeros_like(W), 0\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        grad_W = np.zeros_like(W)\n",
        "        grad_b = 0\n",
        "\n",
        "        # Compute gradients\n",
        "        for i in range(m):\n",
        "            grad_W_i, grad_b_i = grad(W, b, X[i], y_true[i])\n",
        "            grad_W += grad_W_i\n",
        "            grad_b += grad_b_i\n",
        "\n",
        "        # Calculating the Lookahead Factor\n",
        "        lookahead_W = W - momentum_constant * update_W\n",
        "        lookahead_b = b - momentum_constant * update_b\n",
        "\n",
        "        # Dealing with the history\n",
        "        update_W = momentum_constant * update_W + learning_rate * lookahead_W\n",
        "        update_b = momentum_constant * update_b + learning_rate * lookahead_b\n",
        "\n",
        "        # Update weights and bias\n",
        "        W -= update_W\n",
        "        b -= update_b\n",
        "\n",
        "        # Printing the Progress\n",
        "        if epoch % 20 == 0:\n",
        "            print(f\"Epoch: {epoch}\")\n",
        "            print(f\"Weights: {W}\")\n",
        "            print(f\"Bias: {b}\")\n",
        "            print(\"-----------------------------------\")\n",
        "\n",
        "    return W, b"
      ],
      "metadata": {
        "id": "Lmy_l3H97al3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nesterov_accelerated_gradient_descent(X, y_true, epochs = 100, momentum_constant = 0.9, learning_rate = 0.03)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pZwi6xcx6hm_",
        "outputId": "78b0db22-f766-45eb-aea1-e72b194eb84f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 20\n",
            "Weights: [[-0.00135337]\n",
            " [-0.04299686]]\n",
            "Bias: -0.26738001365491054\n",
            "-----------------------------------\n",
            "Epoch: 40\n",
            "Weights: [[0.00035027]\n",
            " [0.01112821]]\n",
            "Bias: 0.0692018284807699\n",
            "-----------------------------------\n",
            "Epoch: 60\n",
            "Weights: [[-8.77888665e-05]\n",
            " [-2.78907887e-03]]\n",
            "Bias: -0.017344149420692834\n",
            "-----------------------------------\n",
            "Epoch: 80\n",
            "Weights: [[2.12704603e-05]\n",
            " [6.75768966e-04]]\n",
            "Bias: 0.004202332904855425\n",
            "-----------------------------------\n",
            "Epoch: 100\n",
            "Weights: [[-4.96050421e-06]\n",
            " [-1.57596721e-04]]\n",
            "Bias: -0.000980030038150811\n",
            "-----------------------------------\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[-4.96050421e-06],\n",
              "        [-1.57596721e-04]]),\n",
              " -0.000980030038150811)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# But how can we find the right **hyperparameters**? Line Search Gradient Descent"
      ],
      "metadata": {
        "id": "JrP-Ub5h9l1M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are different explicit ways of adjusting these values of hyperparameters like tuning the initial or annealing the learning rate & momentum constant.\n",
        "\n",
        "We will focus on Line Search which is an algorithm to find the value of the best hyperparameters."
      ],
      "metadata": {
        "id": "_6zCEUlL91qf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rates = [0.05, 0.1, 0.15, 0.2, 0.25, 0.3]"
      ],
      "metadata": {
        "id": "LewV9bqm-90S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def line_search_gradient_descent(X: np.array, y_true: np.array, learning_rates, epochs = 100, learning_rate = 0.1):\n",
        "    m = X.shape[0]\n",
        "    W, b = init_weights(n_features)\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        grad_W = np.zeros_like(W)\n",
        "        grad_b = 0\n",
        "\n",
        "        # Compute gradients\n",
        "        for i in range(m):\n",
        "            grad_W_i, grad_b_i = grad(W, b, X[i], y_true[i])\n",
        "            grad_W += grad_W_i\n",
        "            grad_b += grad_b_i\n",
        "\n",
        "        min_error = 10000\n",
        "        best_W, best_b = W, b\n",
        "\n",
        "        # Trying different learning rates\n",
        "        for learning_rate in learning_rates:\n",
        "\n",
        "            # Update weights and bias\n",
        "            temp_W = W - learning_rate * grad_W\n",
        "            temp_b = b - learning_rate * grad_b\n",
        "\n",
        "            # Checking if it is better than minimum error\n",
        "            temp_error = error(temp_W, temp_b, X, y_true)\n",
        "            if temp_error < min_error:\n",
        "\n",
        "                # Updating the 'best' parameters\n",
        "                best_W = temp_W\n",
        "                best_b = temp_b\n",
        "                min_error = temp_error\n",
        "\n",
        "        W, b = best_W, best_b\n",
        "\n",
        "        # Printing the Progress\n",
        "        if epoch % 20 == 0:\n",
        "            print(f\"Epoch: {epoch}\")\n",
        "            print(f\"Best Weights: {W}\")\n",
        "            print(f\"Best Bias: {b}\")\n",
        "            print(\"-----------------------------------\")\n",
        "\n",
        "    return W, b"
      ],
      "metadata": {
        "id": "Gw3p-sbW9Yu3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "line_search_gradient_descent(X, y_true, learning_rates, epochs = 100, learning_rate = 0.03)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SVjEC4NH-0dK",
        "outputId": "09d317ce-c193-4694-86e2-83ce707e1563"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 20\n",
            "Best Weights: [[0.51857639]\n",
            " [0.56324472]]\n",
            "Best Bias: [0.96257252]\n",
            "-----------------------------------\n",
            "Epoch: 40\n",
            "Best Weights: [[0.51214154]\n",
            " [0.52646708]]\n",
            "Best Bias: [0.98227999]\n",
            "-----------------------------------\n",
            "Epoch: 60\n",
            "Best Weights: [[0.50663994]\n",
            " [0.51148051]]\n",
            "Best Bias: [0.99167602]\n",
            "-----------------------------------\n",
            "Epoch: 80\n",
            "Best Weights: [[0.50336833]\n",
            " [0.5051078 ]]\n",
            "Best Bias: [0.99610563]\n",
            "-----------------------------------\n",
            "Epoch: 100\n",
            "Best Weights: [[0.50164564]\n",
            " [0.50231116]]\n",
            "Best Bias: [0.99818209]\n",
            "-----------------------------------\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[0.50164564],\n",
              "        [0.50231116]]),\n",
              " array([0.99818209]))"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adaptive Learning Rates"
      ],
      "metadata": {
        "id": "GlQWq7_NBfco"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adaptive learning rate algorithms are a class of optimization algorithms commonly used in training machine learning models, particularly in deep learning. These algorithms dynamically adjust the learning rate during the training process based on the gradients observed so far. This adaptiveness allows them to handle varying gradients and converge faster compared to fixed learning rate methods.\n",
        "\n",
        "Three popular adaptive learning rate algorithms:\n",
        "\n",
        "1.   Adagrad (Adaptive Gradient Algorithm)\n",
        "2.   RMSprop (Root Mean Square Propagation)\n",
        "3.   Adam (Adaptive Moment Estimation)\n",
        "\n"
      ],
      "metadata": {
        "id": "92JlJ-GUB9Ri"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adagrad (Adaptive Gradient Algorithm)"
      ],
      "metadata": {
        "id": "iWvi4YmmCdWK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   Adagrad adapts the learning rates of each parameter individually by scaling them inversely proportional to the square root of the sum of the historical squared gradients.\n",
        "*   It performs larger updates for infrequent parameters and smaller updates for frequent parameters.\n",
        "*   The main drawback is that the learning rates can become too small, causing premature convergence and making it less suitable for deep learning models with many parameters."
      ],
      "metadata": {
        "id": "kyor0V3tCmI-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def adagrad(X: np.array, y_true: np.array, epochs = 100, learning_rate = 0.1, epsilon = 1e-8):\n",
        "    m = X.shape[0]\n",
        "    W, b = init_weights(n_features)\n",
        "    v_W, v_b = np.zeros_like(W), 0\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        grad_W = np.zeros_like(W)\n",
        "        grad_b = 0\n",
        "\n",
        "        # Compute gradients\n",
        "        for i in range(m):\n",
        "            grad_W_i, grad_b_i = grad(W, b, X[i], y_true[i])\n",
        "            grad_W += grad_W_i\n",
        "            grad_b += grad_b_i\n",
        "\n",
        "        # Dealing with the history\n",
        "        v_W += np.square(grad_W)\n",
        "        v_b += grad_b ** 2\n",
        "\n",
        "        # Update weights and bias\n",
        "        W -= (learning_rate / (np.sqrt(v_W + epsilon))) * grad_W\n",
        "        b -= (learning_rate / (np.sqrt(v_b + epsilon))) * grad_b\n",
        "\n",
        "        # Printing the Progress\n",
        "        if epoch % 20 == 0:\n",
        "            print(f\"Epoch: {epoch}\")\n",
        "            print(f\"Weights: {W}\")\n",
        "            print(f\"Bias: {b}\")\n",
        "            print(\"-----------------------------------\")\n",
        "\n",
        "    return W, b"
      ],
      "metadata": {
        "id": "vTZvyGGxBvyh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "adagrad(X, y_true, epochs = 100, learning_rate = 0.03, epsilon = 1e-8)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v9ZhymkXC_BV",
        "outputId": "c8887a45-d76c-4fc0-abf0-df7f15b7e73e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 20\n",
            "Weights: [[0.53025211]\n",
            " [0.37069794]]\n",
            "Bias: [1.04589312]\n",
            "-----------------------------------\n",
            "Epoch: 40\n",
            "Weights: [[0.49850183]\n",
            " [0.42039108]]\n",
            "Bias: [1.03732058]\n",
            "-----------------------------------\n",
            "Epoch: 60\n",
            "Weights: [[0.4920963 ]\n",
            " [0.44843158]]\n",
            "Bias: [1.0273313]\n",
            "-----------------------------------\n",
            "Epoch: 80\n",
            "Weights: [[0.49240405]\n",
            " [0.46583618]]\n",
            "Bias: [1.01917682]\n",
            "-----------------------------------\n",
            "Epoch: 100\n",
            "Weights: [[0.49411794]\n",
            " [0.47712323]]\n",
            "Bias: [1.01319734]\n",
            "-----------------------------------\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[0.49411794],\n",
              "        [0.47712323]]),\n",
              " array([1.01319734]))"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RMSprop (Root Mean Square Propagation)"
      ],
      "metadata": {
        "id": "xpJjZbdyFfur"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   RMSprop addresses the diminishing learning rates problem of Adagrad by using an exponentially decaying average of squared gradients.\n",
        "*   Instead of accumulating all past squared gradients, it only considers a moving average of recent gradients, which prevents the learning rates from becoming overly small.\n",
        "*   RMSprop is basically less agressive on the decay.\n",
        "\n"
      ],
      "metadata": {
        "id": "sAbwIoTeGNON"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rmsprop(X: np.array, y_true: np.array, epochs = 100, learning_rate = 0.1, beta = 0.9, epsilon = 1e-8):\n",
        "    m = X.shape[0]\n",
        "    W, b = init_weights(n_features)\n",
        "    v_W, v_b = np.zeros_like(W), 0\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        grad_W = np.zeros_like(W)\n",
        "        grad_b = 0\n",
        "\n",
        "        # Compute gradients\n",
        "        for i in range(m):\n",
        "            grad_W_i, grad_b_i = grad(W, b, X[i], y_true[i])\n",
        "            grad_W += grad_W_i\n",
        "            grad_b += grad_b_i\n",
        "\n",
        "        # Dealing with the history\n",
        "        v_W = (beta * v_W) + ((1 - beta) * np.square(grad_W))\n",
        "        v_b = (beta * v_b) + ((1 - beta) * (grad_b ** 2))\n",
        "\n",
        "        # Update weights and bias\n",
        "        W -= (learning_rate / (np.sqrt(v_W + epsilon))) * grad_W\n",
        "        b -= (learning_rate / (np.sqrt(v_b + epsilon))) * grad_b\n",
        "\n",
        "        # Printing the Progress\n",
        "        if epoch % 20 == 0:\n",
        "            print(f\"Epoch: {epoch}\")\n",
        "            print(f\"Weights: {W}\")\n",
        "            print(f\"Bias: {b}\")\n",
        "            print(\"-----------------------------------\")\n",
        "\n",
        "    return W, b"
      ],
      "metadata": {
        "id": "jZSzPoiaFcE3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rmsprop(X, y_true, epochs = 100, learning_rate = 0.1, beta = 0.9, epsilon = 1e-8)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_fThg4mrGFb6",
        "outputId": "e3ec1db0-8890-433f-eb88-b45e66cab236"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 20\n",
            "Weights: [[0.46584914]\n",
            " [0.46732211]]\n",
            "Bias: [1.0323247]\n",
            "-----------------------------------\n",
            "Epoch: 40\n",
            "Weights: [[0.58225138]\n",
            " [0.5817491 ]]\n",
            "Bias: [1.08146301]\n",
            "-----------------------------------\n",
            "Epoch: 60\n",
            "Weights: [[0.55598221]\n",
            " [0.55552687]]\n",
            "Bias: [1.0445546]\n",
            "-----------------------------------\n",
            "Epoch: 80\n",
            "Weights: [[0.56223659]\n",
            " [0.56191189]]\n",
            "Bias: [1.04990612]\n",
            "-----------------------------------\n",
            "Epoch: 100\n",
            "Weights: [[0.56130444]\n",
            " [0.56099221]]\n",
            "Bias: [1.04869945]\n",
            "-----------------------------------\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[0.56130444],\n",
              "        [0.56099221]]),\n",
              " array([1.04869945]))"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adam (Adaptive Moment Estimation)"
      ],
      "metadata": {
        "id": "Aoa7KVDONP5D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   Adam combines the ideas of momentum optimization and RMSprop. (Best of both Worlds)\n",
        "*   It maintains both a decaying average of past gradients (like momentum) and a decaying average of past squared gradients (like RMSprop).\n",
        "*   Adam also incorporates bias correction to account for the initial bias of estimates during the early training stages.\n",
        "*   It has been widely adopted due to its good performance across various tasks and its ability to handle noisy or sparse gradients.\n",
        "\n"
      ],
      "metadata": {
        "id": "L1tIMv1lNRNQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def adam(X: np.array, y_true: np.array, epochs = 100, learning_rate = 0.1, beta_1 = 0.9, beta_2 = 0.999, epsilon = 1e-8):\n",
        "    m = X.shape[0]\n",
        "    W, b = init_weights(n_features)\n",
        "    m_W, m_b = np.zeros_like(W), 0\n",
        "    v_W, v_b = np.zeros_like(W), 0\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        grad_W = np.zeros_like(W)\n",
        "        grad_b = 0\n",
        "\n",
        "        # Compute gradients\n",
        "        for i in range(m):\n",
        "            grad_W_i, grad_b_i = grad(W, b, X[i], y_true[i])\n",
        "            grad_W += grad_W_i\n",
        "            grad_b += grad_b_i\n",
        "\n",
        "        # Dealing with the first moment history\n",
        "        m_W = (beta_1 * m_W) + ((1 - beta_1) * grad_W)\n",
        "        m_b = (beta_1 * m_b) + ((1 - beta_1) * grad_b)\n",
        "\n",
        "        # Dealing with the second moment history\n",
        "        v_W = (beta_2 * v_W) + ((1 - beta_2) * np.square(grad_W))\n",
        "        v_b = (beta_2 * v_b) + ((1 - beta_2) * (grad_b ** 2))\n",
        "\n",
        "        # Bias Correction\n",
        "        m_w_hat = m_W / (1 - (beta_1 ** (i)))\n",
        "        m_b_hat = m_b / (1 - (beta_1 ** (i)))\n",
        "\n",
        "        v_w_hat = v_W / (1 - (beta_2 ** (i)))\n",
        "        v_b_hat = v_b / (1 - (beta_2 ** (i)))\n",
        "\n",
        "        # Update weights and bias\n",
        "        W -= (learning_rate / (np.sqrt(v_w_hat + epsilon))) * m_w_hat\n",
        "        b -= (learning_rate / (np.sqrt(v_b_hat + epsilon))) * m_b_hat\n",
        "\n",
        "        # Printing the Progress\n",
        "        if epoch % 20 == 0:\n",
        "            print(f\"Epoch: {epoch}\")\n",
        "            print(f\"Weights: {W}\")\n",
        "            print(f\"Bias: {b}\")\n",
        "            print(\"-----------------------------------\")\n",
        "\n",
        "    return W, b"
      ],
      "metadata": {
        "id": "BPz5blOjNNSs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "adam(X, y_true, epochs = 100, learning_rate = 0.1, beta_1 = 0.9, beta_2 = 0.999, epsilon = 1e-8)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iboe5NRyPNb_",
        "outputId": "2b4bf70b-d022-428a-d23c-4a3b22184fd6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 20\n",
            "Weights: [[0.53514516]\n",
            " [0.49306076]]\n",
            "Bias: [0.99195921]\n",
            "-----------------------------------\n",
            "Epoch: 40\n",
            "Weights: [[0.51211503]\n",
            " [0.4784143 ]]\n",
            "Bias: [1.00549875]\n",
            "-----------------------------------\n",
            "Epoch: 60\n",
            "Weights: [[0.50256948]\n",
            " [0.49368503]]\n",
            "Bias: [1.0015345]\n",
            "-----------------------------------\n",
            "Epoch: 80\n",
            "Weights: [[0.50015296]\n",
            " [0.50063332]]\n",
            "Bias: [0.99943548]\n",
            "-----------------------------------\n",
            "Epoch: 100\n",
            "Weights: [[0.49960328]\n",
            " [0.50088987]]\n",
            "Bias: [0.99970314]\n",
            "-----------------------------------\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[0.49960328],\n",
              "        [0.50088987]]),\n",
              " array([0.99970314]))"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    }
  ]
}
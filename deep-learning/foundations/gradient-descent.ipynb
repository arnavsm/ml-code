{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "gC-FvreuLtT4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "HQ41jcpVKTus"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What is Gradient Descent?"
      ],
      "metadata": {
        "id": "yDnslN5xX5pP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gradient descent is a fundamental optimization algorithm used in machine learning and optimization problems to minimize the cost function or loss function. The concept revolves around iteratively adjusting the parameters of a model in the direction of the steepest descent of the gradient of the cost function. The gradient represents the direction of the steepest increase in the function. In each iteration, the algorithm calculates the gradient of the cost function with respect to the parameters, and then updates the parameters in the opposite direction of the gradient by a certain step size known as the learning rate. This process continues until convergence, where the gradient becomes nearly zero, indicating that the algorithm has reached a local minimum."
      ],
      "metadata": {
        "id": "MeMMtifcWO2a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Selecting the Functions"
      ],
      "metadata": {
        "id": "Gi57LICHL1Q_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Function to Replicate"
      ],
      "metadata": {
        "id": "WTQFTAhrZxVB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The Sigmoid Function**\n",
        "\n",
        "The sigmoid function is a mathematical function that maps any real-valued number to a value between 0 and 1. It introduces non-linearity to the network and enables it to learn complex patterns in the data.\n",
        "\n",
        "**Properties**\n",
        "\n",
        "- **Range**: The output of the sigmoid function is always between 0 and 1. As \\( x \\) approaches negative infinity, the output approaches 0, and as \\( x \\) approaches positive infinity, the output approaches 1.\n",
        "- **Smoothness**: The sigmoid function is smooth and differentiable everywhere, which makes it suitable for optimization algorithms such as gradient descent.\n",
        "- **S-shaped curve**: The graph of the sigmoid function resembles the letter \"S\", hence the name \"sigmoid\". This shape introduces non-linearity to the network, allowing it to model complex relationships in the data.\n"
      ],
      "metadata": {
        "id": "xU5GFLFthuUF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(W: np.array, b: np.array, X: np.array):\n",
        "    return (1 / (1 + np.exp(-(np.dot(X, W) + b))))"
      ],
      "metadata": {
        "id": "JfamfYVvMLjx"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loss Function"
      ],
      "metadata": {
        "id": "nCEWuet7Z1y_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mean squared error (MSE) is a fundamental metric used in statistics and machine learning to quantify the average squared difference between the actual values and the predicted values. It is calculated by taking the average of the squared differences between the predicted and true values for each data point.\n",
        "\n",
        "MSE is favored for its mathematical properties, such as being non-negative and sensitive to the magnitude of errors, making it a widely adopted measure for evaluating model performance and guiding optimization efforts."
      ],
      "metadata": {
        "id": "bBMDORLtZ6Ji"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def error(W: np.array, b: np.array, X: np.array, y_true: np.array):\n",
        "    m = X.shape[0]\n",
        "\n",
        "    y_hat = sigmoid(W, b, X)\n",
        "\n",
        "    cost = np.sum((y_hat - y_true)**2, axis = 0) / m\n",
        "\n",
        "    return cost"
      ],
      "metadata": {
        "id": "5pLerj-YZ5qM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generating the Data"
      ],
      "metadata": {
        "id": "tGpD1pgpLvui"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Taking number of features as 5 and instances as 100\n",
        "n_features = 3\n",
        "m = 100"
      ],
      "metadata": {
        "id": "mCJkrsX9MMyw"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "W_true = np.full((n_features, 1), 0.5)\n",
        "b_true = 1\n",
        "\n",
        "# We are only focused on the algorithm, so we generate this randomly\n",
        "\n",
        "X = np.random.rand(m, n_features)\n",
        "\n",
        "y_true = sigmoid(W_true, b_true, X)\n",
        "y_true.shape, X.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8b1gOtlQYLv8",
        "outputId": "d30dc065-775f-4754-a5d2-bdfe48bb3804"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((100, 1), (100, 3))"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Calculating Gradients"
      ],
      "metadata": {
        "id": "maWe3ao4L2r4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def grad(W: np.array, b: np.array, X: np.array, y_true: np.array):\n",
        "    m = X.shape[0]\n",
        "    n_features = X.shape[1]\n",
        "\n",
        "    # Calculating predicted y\n",
        "    y_hat = sigmoid(W, b, X)\n",
        "\n",
        "    # Initializing gradients\n",
        "    grad_W = np.zeros_like(W)\n",
        "    grad_b = 0\n",
        "\n",
        "    for i in range(m):\n",
        "        for j in range(n_features):\n",
        "            grad_W[j] += np.dot((y_hat[i] - y_true[i]) * (y_hat[i])* (1 - y_hat[i]), X[i][j])\n",
        "        grad_b += (y_hat[i] - y_true[i]) * (y_hat[i]) * (1 - y_hat[i])\n",
        "\n",
        "    return grad_W, grad_b"
      ],
      "metadata": {
        "id": "TZ6DVtyGML5b"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gradient Descent Loop"
      ],
      "metadata": {
        "id": "LoGX_XjUL5P9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_descent(W: np.array, b: np.array, X: np.array, y_true: np.array, epochs = 100, learning_rate = 0.1):\n",
        "    m = X.shape[0]\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "\n",
        "        # Compute gradients\n",
        "        grad_W, grad_b = grad(W, b, X, y_true)\n",
        "\n",
        "        # Update weights and bias\n",
        "        W -= learning_rate * grad_W\n",
        "        b -= learning_rate * grad_b\n",
        "\n",
        "        # Printing the Progress\n",
        "        if epoch % 10 == 0:\n",
        "            print(f\"Epoch: {epoch}\")\n",
        "            print(f\"Weights: {W}\")\n",
        "            print(f\"Bias: {b}\")\n",
        "            print(\"-----------------------------------\")\n",
        "\n",
        "    return W, b"
      ],
      "metadata": {
        "id": "bDvg2y3YMMc1"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "W = np.random.rand(n_features, 1)\n",
        "b = 1"
      ],
      "metadata": {
        "id": "InbQupv0ikQs"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gradient_descent(W, b, X, y_true, epochs = 100, learning_rate = 0.03)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gTYrsB3sihCc",
        "outputId": "f54dab21-1ee1-4c18-db7e-04da458e8977"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0\n",
            "Weights: [[0.23151505]\n",
            " [0.54758624]\n",
            " [0.54064995]]\n",
            "Bias: [1.00461619]\n",
            "-----------------------------------\n",
            "Epoch: 10\n",
            "Weights: [[0.25468119]\n",
            " [0.55742402]\n",
            " [0.54963451]]\n",
            "Bias: [1.03225451]\n",
            "-----------------------------------\n",
            "Epoch: 20\n",
            "Weights: [[0.26904467]\n",
            " [0.55939198]\n",
            " [0.55098635]]\n",
            "Bias: [1.04264798]\n",
            "-----------------------------------\n",
            "Epoch: 30\n",
            "Weights: [[0.27981704]\n",
            " [0.55841785]\n",
            " [0.54951411]]\n",
            "Bias: [1.04639438]\n",
            "-----------------------------------\n",
            "Epoch: 40\n",
            "Weights: [[0.28893044]\n",
            " [0.55631601]\n",
            " [0.54698947]]\n",
            "Bias: [1.04742517]\n",
            "-----------------------------------\n",
            "Epoch: 50\n",
            "Weights: [[0.29716913]\n",
            " [0.55381435]\n",
            " [0.54412192]]\n",
            "Bias: [1.04732672]\n",
            "-----------------------------------\n",
            "Epoch: 60\n",
            "Weights: [[0.3048667 ]\n",
            " [0.55121394]\n",
            " [0.54120328]]\n",
            "Bias: [1.0467613]\n",
            "-----------------------------------\n",
            "Epoch: 70\n",
            "Weights: [[0.31217081]\n",
            " [0.54863954]\n",
            " [0.53835272]]\n",
            "Bias: [1.0460091]\n",
            "-----------------------------------\n",
            "Epoch: 80\n",
            "Weights: [[0.31915063]\n",
            " [0.54614157]\n",
            " [0.53561673]]\n",
            "Bias: [1.04518903]\n",
            "-----------------------------------\n",
            "Epoch: 90\n",
            "Weights: [[0.32584181]\n",
            " [0.54373887]\n",
            " [0.53301098]]\n",
            "Bias: [1.04435139]\n",
            "-----------------------------------\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[0.33163468],\n",
              "        [0.54166255],\n",
              "        [0.53077951]]),\n",
              " array([1.04360017]))"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    }
  ]
}
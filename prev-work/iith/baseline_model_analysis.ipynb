{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Installs"
      ],
      "metadata": {
        "id": "L0ek_vF8eMoN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install einops ptflops"
      ],
      "metadata": {
        "id": "SsELziIbeOdr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1889d8c0-1ad4-4479-8a29-88f93bce92ef"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (0.8.0)\n",
            "Requirement already satisfied: ptflops in /usr/local/lib/python3.10/dist-packages (0.7.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from ptflops) (2.3.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->ptflops) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->ptflops) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->ptflops) (1.13.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->ptflops) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->ptflops) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->ptflops) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->ptflops) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->ptflops) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->ptflops) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->ptflops) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->ptflops) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->ptflops) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->ptflops) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->ptflops) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->ptflops) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch->ptflops) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->ptflops) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->ptflops) (2.3.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->ptflops) (12.5.82)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->ptflops) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->ptflops) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "rrnKI51w-9rE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from ptflops import get_model_complexity_info\n",
        "import gc\n",
        "import math\n",
        "from einops import rearrange"
      ],
      "metadata": {
        "id": "9uZXasIe9Ab7"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device"
      ],
      "metadata": {
        "id": "ucIQw_moJMxP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "31eb0458-93b8-4b99-9f68-937331127de9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cpu'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utils"
      ],
      "metadata": {
        "id": "aTP9UEtENSgm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clear_model_from_memory(model: nn.Module): # clearing memory\n",
        "    del model\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "yifrGnzjM3qM"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def count_parameters_with_commas(model: nn.Module) -> str: # no. of params\n",
        "    total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    formatted_params = f\"{total_params:,}\"\n",
        "    return formatted_params"
      ],
      "metadata": {
        "id": "nGgygwue8hTI"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model_size_mb(model: nn.Module): # params size in mb\n",
        "    total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    size_in_bytes = total_params * 4\n",
        "    size_in_mb = size_in_bytes / (1024 ** 2)\n",
        "    return size_in_mb"
      ],
      "metadata": {
        "id": "bxebqlgaA_0d"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def count_model_flops(model: nn.Module, input_size=(3, 224, 224), print_results=True):\n",
        "    try:\n",
        "        macs, params = get_model_complexity_info(\n",
        "            model, input_size, as_strings=False, print_per_layer_stat=False, verbose=False\n",
        "        )\n",
        "\n",
        "        gflops = macs / 1e9  # Convert MACs to GFLOPs\n",
        "\n",
        "        if print_results:\n",
        "            print(f'Computational complexity: {gflops:.3f} GFLOPs')\n",
        "            print(f'Number of parameters: {params / 1e6:.3f} M')\n",
        "\n",
        "        return gflops, params\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {str(e)}\")\n",
        "        return None, None"
      ],
      "metadata": {
        "id": "tY826J5hKwD_"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model 1 (4 Layers)"
      ],
      "metadata": {
        "id": "SVH6DF1Q-_Ac"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "o3WxOvp46DBY"
      },
      "outputs": [],
      "source": [
        "class PatchEmbedding(nn.Module):\n",
        "    def __init__(self, in_channels=3, patch_size=16, emb_dim=1024):\n",
        "        super(PatchEmbedding, self).__init__()\n",
        "        self.patch_size = patch_size\n",
        "        self.proj = nn.Conv2d(in_channels, emb_dim, kernel_size=patch_size, stride=patch_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.proj(x)  # (B, emb_dim, H/patch_size, W/patch_size)\n",
        "        x = rearrange(x, 'b c h w -> b (h w) c')  # (B, num_patches, emb_dim)\n",
        "        return x\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, emb_dim, num_heads, mlp_ratio=4., dropout=0.1):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.norm1 = nn.LayerNorm(emb_dim)\n",
        "        self.attn = nn.MultiheadAttention(emb_dim, num_heads, dropout=dropout)\n",
        "        self.norm2 = nn.LayerNorm(emb_dim)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(emb_dim, int(mlp_ratio * emb_dim)),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(int(mlp_ratio * emb_dim), emb_dim),\n",
        "        )\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.dropout(self.attn(self.norm1(x), self.norm1(x), self.norm1(x))[0])\n",
        "        x = x + self.dropout(self.mlp(self.norm2(x)))\n",
        "        return x\n",
        "\n",
        "class ProgressiveUpsamplingTransformer(nn.Module):\n",
        "    def __init__(self, in_channels=3, emb_dim=768, num_heads=12, num_layers=4, num_classes=30):\n",
        "        super(ProgressiveUpsamplingTransformer, self).__init__()\n",
        "        self.patch_size = 16\n",
        "        self.emb_dim = emb_dim\n",
        "        self.patch_embed = PatchEmbedding(in_channels, self.patch_size, emb_dim)\n",
        "\n",
        "        self.transformer_blocks = nn.ModuleList([\n",
        "            TransformerBlock(emb_dim, num_heads) for _ in range(num_layers)\n",
        "        ])\n",
        "        self.final_proj = nn.Conv2d(emb_dim, num_classes, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, C, H, W = x.shape\n",
        "        x = self.patch_embed(x)  # (B, num_patches, emb_dim)\n",
        "        H_p, W_p = H // self.patch_size, W // self.patch_size\n",
        "        x = rearrange(x, 'b (h w) c -> b c h w', h=H_p, w=W_p)  # (B, emb_dim, H_p, W_p)\n",
        "\n",
        "        for block in self.transformer_blocks:\n",
        "            x = rearrange(x, 'b c h w -> b (h w) c')  # (B, num_patches, emb_dim)\n",
        "            x = block(x)  # (B, num_patches, emb_dim)\n",
        "            x = rearrange(x, 'b (h w) c -> b c h w', h=H_p, w=W_p)  # (B, emb_dim, H_p, W_p)\n",
        "            H_p, W_p = H_p * 2, W_p * 2\n",
        "            x = F.interpolate(x, size=(H_p, W_p), mode='bilinear', align_corners=False)  # Upsample\n",
        "\n",
        "        x = self.final_proj(x)  # (B, num_classes, H, W)\n",
        "        x = F.interpolate(x, size=(H, W), mode='bilinear', align_corners=False)  # Ensure the output size is same as input\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "model1 = ProgressiveUpsamplingTransformer().to(device)\n",
        "input_tensor = torch.randn(1, 3, 224, 224).to(device)  # Example input tensor\n",
        "output = model1(input_tensor)\n",
        "print(output.shape)"
      ],
      "metadata": {
        "id": "RSQ6775OJi26",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c102999-12de-41c8-c6d7-b2649b052224"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 30, 224, 224])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model 2 (12 Layers Total)"
      ],
      "metadata": {
        "id": "bHA0MV1bFD5g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PatchEmbedding(nn.Module):\n",
        "    def __init__(self, in_channels=3, patch_size=16, emb_dim=768):\n",
        "        super(PatchEmbedding, self).__init__()\n",
        "        self.patch_size = patch_size\n",
        "        self.proj = nn.Conv2d(in_channels, emb_dim, kernel_size=patch_size, stride=patch_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.proj(x)  # (B, emb_dim, H/patch_size, W/patch_size)\n",
        "        x = rearrange(x, 'b c h w -> b (h w) c')  # (B, num_patches, emb_dim)\n",
        "        return x\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, emb_dim, num_heads, mlp_ratio=4., dropout=0.1):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.norm1 = nn.LayerNorm(emb_dim)\n",
        "        self.attn = nn.MultiheadAttention(emb_dim, num_heads, dropout=dropout)\n",
        "        self.norm2 = nn.LayerNorm(emb_dim)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(emb_dim, int(mlp_ratio * emb_dim)),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(int(mlp_ratio * emb_dim), emb_dim),\n",
        "        )\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.norm1(x)\n",
        "        x = x + self.dropout(self.attn(x, x, x)[0])\n",
        "        x = self.norm2(x)\n",
        "        x = x + self.dropout(self.mlp(x))\n",
        "        return x\n",
        "\n",
        "class UpsamplingTransformer(nn.Module):\n",
        "    def __init__(self, in_channels=3, emb_dim=768, num_heads=12, num_classes=21):\n",
        "        super(UpsamplingTransformer, self).__init__()\n",
        "        self.patch_size = 16\n",
        "        self.emb_dim = emb_dim\n",
        "        self.patch_embed = PatchEmbedding(in_channels, self.patch_size, emb_dim)\n",
        "\n",
        "        self.encoder_blocks = nn.ModuleList([\n",
        "            TransformerBlock(emb_dim, num_heads) for _ in range(4)\n",
        "        ])\n",
        "        self.upsampling_blocks = nn.ModuleList([\n",
        "            TransformerBlock(emb_dim, num_heads) for _ in range(4)\n",
        "        ])\n",
        "        self.additional_blocks = nn.ModuleList([\n",
        "            TransformerBlock(emb_dim, num_heads) for _ in range(4)\n",
        "        ])\n",
        "        self.final_proj = nn.Conv2d(emb_dim, num_classes, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, C, H, W = x.shape\n",
        "        x = self.patch_embed(x)  # (B, num_patches, emb_dim)\n",
        "        H_p, W_p = H // self.patch_size, W // self.patch_size\n",
        "        x = rearrange(x, 'b (h w) c -> b c h w', h=H_p, w=W_p)  # (B, emb_dim, H_p, W_p)\n",
        "\n",
        "        for block in self.encoder_blocks:\n",
        "            x = rearrange(x, 'b c h w -> b (h w) c')  # (B, num_patches, emb_dim)\n",
        "            x = block(x)  # (B, num_patches, emb_dim)\n",
        "            x = rearrange(x, 'b (h w) c -> b c h w', h=H_p, w=W_p)  # (B, emb_dim, H_p, W_p)\n",
        "\n",
        "        for block in self.upsampling_blocks:\n",
        "            H_p, W_p = H_p * 2, W_p * 2\n",
        "            x = F.interpolate(x, size=(H_p, W_p), mode='bilinear', align_corners=False)  # Upsample\n",
        "            x = rearrange(x, 'b c h w -> b (h w) c')  # (B, num_patches, emb_dim)\n",
        "            x = block(x)  # (B, num_patches, emb_dim)\n",
        "            x = rearrange(x, 'b (h w) c -> b c h w', h=H_p, w=W_p)  # (B, emb_dim, H_p, W_p)\n",
        "\n",
        "        for block in self.additional_blocks:\n",
        "            x = rearrange(x, 'b c h w -> b (h w) c')  # (B, num_patches, emb_dim)\n",
        "            x = block(x)  # (B, num_patches, emb_dim)\n",
        "            x = rearrange(x, 'b (h w) c -> b c h w', h=H_p, w=W_p)  # (B, emb_dim, H_p, W_p)\n",
        "\n",
        "        x = self.final_proj(x)  # (B, num_classes, H_p, W_p)\n",
        "        x = F.interpolate(x, size=(H, W), mode='bilinear', align_corners=False)  # Ensure the output size is same as input\n",
        "        return x"
      ],
      "metadata": {
        "id": "Iuq5ZumCvUtM"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "model2 = UpsamplingTransformer().to(device)\n",
        "input_tensor = torch.randn(1, 3, 224, 224).to(device)  # Example input tensor\n",
        "output = model2(input_tensor)\n",
        "print(output.shape)"
      ],
      "metadata": {
        "id": "ifmV7PFRwd39"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Size"
      ],
      "metadata": {
        "id": "8iv9f-5hFKSr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "count_parameters_with_commas(model1)"
      ],
      "metadata": {
        "id": "MpTQVPMl9kQ7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ce9111c3-4488-48ce-a1bf-73ea28317ecf"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'28,965,150'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_size_mb(model1)"
      ],
      "metadata": {
        "id": "cmgJ4dn9BZ8R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f16a1e16-e51a-49bd-dfcc-5bf5e6f30b3f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "110.49327850341797"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "count_parameters_with_commas(model2)"
      ],
      "metadata": {
        "id": "X5ESViTp9wjO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_size_mb(model2)"
      ],
      "metadata": {
        "id": "5qda5Ym_FS6x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FLOPS"
      ],
      "metadata": {
        "id": "iIX4z68HC6j6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "count_model_flops(model1)"
      ],
      "metadata": {
        "id": "8u4Fua-_DEVW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90ebe98b-c09d-4060-93f6-649c4a9aa151"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computational complexity: 119.524 GFLOPs\n",
            "Number of parameters: 28.965 M\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(119.524113968, 28965150)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clear_model_from_memory(model1)"
      ],
      "metadata": {
        "id": "n0oeq-vxMU6b"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count_model_flops(model2)"
      ],
      "metadata": {
        "id": "aKXAz4VSE1VU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `mingrad` package is a minimal implementation of automatic differentiation and computational graphs in Python, inspired by Andrej Karpathy's `micrograd` library. It serves as an educational resource to understand the fundamental concepts behind modern deep learning frameworks, such as PyTorch and TensorFlow.\n",
    "\n",
    "Karpathy's `micrograd` is a minimalist autograd engine written in Python, designed to illustrate the core principles of backpropagation and automatic differentiation. The `mingrad` package follows a similar philosophy, providing a lightweight and easy-to-understand codebase that demystifies the inner workings of these essential concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Automatic Differentiation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Automatic differentiation** (AD) is a computational technique used to evaluate the derivatives of functions efficiently and accurately. Unlike symbolic differentiation, which can become cumbersome and complex, or numerical differentiation, which can be imprecise, AD leverages the chain rule of calculus to systematically apply differentiation operations to the basic arithmetic operations and elementary functions in a program. This process yields exact derivatives, is highly efficient, and works for functions of arbitrary complexity, making it a powerful tool in various fields such as machine learning, optimization, and scientific computing.\n",
    "\n",
    "Automatic differentiation (AD) comes in two primary modes: **forward mode** and **reverse mode**, both based on the chain rule. \n",
    "1. In **forward mode**, the computation propagates from the input variables to the output variables, computing the derivative of each intermediate variable with respect to the input variables. This mode is efficient for functions with a small number of inputs and a larger number of outputs. For a function $f(x_1, x_2, \\ldots, x_n)$, forward mode calculates the derivative of each $x_i$ through each step of the function until the final output. \n",
    "\n",
    "2. On the other hand, **reverse mode** propagates from the output variables back to the input variables, computing the derivative of the output variable with respect to each intermediate variable. This mode is efficient for functions with a large number of inputs and a single output. For a function $f(x_1, x_2, \\ldots, x_n)$, reverse mode calculates the derivatives by first evaluating the function normally and then propagating gradients from the output back through each intermediate step to the inputs. \n",
    "\n",
    "Forward mode is typically used when the number of inputs is small, while reverse mode is more suited for functions with a single output and many inputs, such as in training neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Important Terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Computational Graphs**: \n",
    "Computational graphs are a way to represent mathematical operations as a series of interconnected nodes. Each node represents a specific operation, and the edges between nodes represent the flow of data. This graph-like structure allows for efficient computation and, more importantly, enables automatic differentiation.\n",
    "\n",
    "- **Gradients**: In the context of neural networks, gradients refer to the partial derivatives of the loss function with respect to the network's weights and biases. These gradients represent the direction and rate of change of the loss function, and they are crucial for updating the network's parameters during training.\n",
    "\n",
    "- **Forward and Backward Propagation**: During the training process, inputs are passed through the neural network in the forward propagation phase to obtain predictions or outputs. These outputs are then compared to the true labels, and the errors are propagated back through the network in the backward propagation phase, computing gradients of the loss function with respect to the network's weights using the chain rule of calculus.\n",
    "\n",
    "- **Gradient Descent**: The gradients computed during backward propagation are used by the gradient descent optimization algorithm to iteratively adjust the network's weights in the direction that minimizes the loss function. This process continues until the loss is minimized to an acceptable level, and the network's predictions become accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To install the `mingrad` package, you can follow one of the two methods below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation via pip\n",
    "\n",
    "If the `mingrad` package is available on the Python Package Index (PyPI), you can install it using the pip package installer. Open your terminal or command prompt and run the following command:\n",
    "\n",
    "```bash\n",
    "pip install mingrad\n",
    "```\n",
    "\n",
    "This will download and install the latest stable version of the `mingrad` package and its dependencies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation from Source\n",
    "\n",
    "Alternatively, if you want to install the package from source, you can clone the `mingrad` repository from GitHub. Follow these steps:\n",
    "\n",
    "```bash\n",
    "# Clone the repository\n",
    "git clone https://github.com/yourusername/mingrad.git\n",
    "\n",
    "# Navigate to the project directory\n",
    "cd mingrad\n",
    "\n",
    "# Install the package in editable mode\n",
    "pip install -e .\n",
    "```\n",
    "\n",
    "The -e flag installs the package in editable mode, allowing you to make changes to the source code and have them immediately reflected in your Python environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After following either of these installation methods, you should be able to import and use the mingrad package in your Python scripts or interactive sessions.\n",
    "\n",
    "```python\n",
    "import mingrad\n",
    "\n",
    "# Use the mingrad package\n",
    "...\n",
    "```\n",
    "\n",
    "Make sure to check the repository's documentation for any additional installation requirements or instructions specific to your use case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import the necessary classes\n",
    "# Assuming the Variable class is already defined in the workspace as provided earlier\n",
    "from mingrad.engine import Variable\n",
    "\n",
    "# Step 2: Create a simple computational graph\n",
    "x = Variable(2.0)  # create a Variable with value 2.0\n",
    "y = Variable(3.0)  # create a Variable with value 3.0\n",
    "\n",
    "# Perform operations to create a computational graph\n",
    "z = x**2 + y  # z = x^2 + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Forward propagation\n",
    "print(\"Forward Propagation:\")\n",
    "print(f\"x: {x}\")\n",
    "print(f\"y: {y}\")\n",
    "print(f\"z: {z}\")\n",
    "\n",
    "# Step 3: Backward propagation\n",
    "z.backward()\n",
    "\n",
    "print(\"\\nBackward Propagation:\")\n",
    "print(f\"x.grad: {x.grad}\")  # Should print the gradient of z with respect to x\n",
    "print(f\"y.grad: {y.grad}\")  # Should print the gradient of z with respect to y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mingrad.nn import MLP\n",
    "\n",
    "# Create a toy dataset\n",
    "X = [[2.0, 3.0], [-1.0, -2.0], [1.5, 2.0], [-1.5, -2.5]]\n",
    "y = [1.0, -1.0, 1.0, -1.0]\n",
    "\n",
    "# Initialize the network\n",
    "mlp = MLP(2, [3, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the network on the toy dataset\n",
    "learning_rate = 0.01\n",
    "for epoch in range(100):  # number of epochs\n",
    "    for xi, yi in zip(X, y):\n",
    "        # Forward pass\n",
    "        y_pred = mlp(xi)\n",
    "        \n",
    "        # Calculate loss (mean squared error)\n",
    "        loss = (y_pred - yi) ** 2\n",
    "        \n",
    "        # Backward pass\n",
    "        mlp.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # Update parameters\n",
    "        for p in mlp.parameters():\n",
    "            p.data -= learning_rate * p.grad\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {loss.data}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing final predictions\n",
    "print(\"\\nFinal Predictions:\")\n",
    "for xi in X:\n",
    "    y_pred = mlp(xi)\n",
    "    print(f\"Input: {xi}, Predicted: {y_pred.data}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This structure covers the essential aspects of introducing the `mingrad` package, demonstrating its core functionality, and guiding users through the process of creating and training simple neural networks. You can expand or modify the content based on your specific requirements and any additional features or examples you want to include."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.1.-1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

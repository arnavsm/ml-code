{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":84705,"databundleVersionId":9755748,"sourceType":"competition"}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Setup","metadata":{"_uuid":"2a0bea65-05ae-4330-bd10-da2723aa9454","_cell_guid":"9958dd4f-23ea-4be0-a516-f725baad335d","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"## Installs","metadata":{"_uuid":"6464bda1-8c57-45e6-95ec-9127285243dc","_cell_guid":"2fd287b2-ee94-4daa-ac8c-e9eb350320f9","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"!pip install git+https://github.com/openai/CLIP.git","metadata":{"_uuid":"781f0312-aaac-4f38-a18d-b419bdadc7ab","_cell_guid":"f494d340-a150-454e-91a0-62cf0ecd1a26","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-11-05T05:58:53.633840Z","iopub.execute_input":"2024-11-05T05:58:53.634760Z","iopub.status.idle":"2024-11-05T05:59:07.153157Z","shell.execute_reply.started":"2024-11-05T05:58:53.634719Z","shell.execute_reply":"2024-11-05T05:59:07.151916Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Imports","metadata":{"_uuid":"b1549789-6022-470c-834d-89f168213d09","_cell_guid":"f8c099e9-3ad7-4bf6-b461-f1b28f06298a","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"import os\nimport re\nimport collections\nimport random\nimport json\nimport typing\nimport warnings\nfrom collections import defaultdict\nfrom typing import Dict, List, Tuple\nfrom pathlib import Path\nfrom logging import getLogger, Logger, INFO, StreamHandler, FileHandler, Formatter\n\nfrom tqdm import tqdm\nimport pandas as pd\nimport numpy as np\nfrom PIL import Image\n\nimport torch\nfrom torch import nn, optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.nn import functional as F\n\nimport torchvision\nfrom torchvision import transforms\nimport torchmetrics\n\nimport clip\nwarnings.filterwarnings('ignore')\nprint(\"Libraries Imported Successfully!\")","metadata":{"_uuid":"74aaa9ad-798b-497e-a22d-3b8f46d55d8f","_cell_guid":"7090cfc8-102f-4520-859c-ddbfdc4a80b4","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-11-05T05:59:07.155628Z","iopub.execute_input":"2024-11-05T05:59:07.156592Z","iopub.status.idle":"2024-11-05T05:59:07.165639Z","shell.execute_reply.started":"2024-11-05T05:59:07.156526Z","shell.execute_reply":"2024-11-05T05:59:07.164686Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Utility","metadata":{"_uuid":"0a2fa4ab-d70a-4200-9b82-56b3193aa723","_cell_guid":"68267ebf-d87e-4b86-a3bc-4b62c9e116a2","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"def save_model(model: nn.Module, target_dir: str, model_name: str):\n    \"\"\"\n    Save the model's state dictionary to a given directory.\n    \"\"\"\n    target_dir_path = Path(target_dir)\n    target_dir_path.mkdir(parents=True, exist_ok=True)  # Create the directory if it doesn't exist\n\n    assert model_name.endswith(\".pth\") or model_name.endswith(\".pt\"), \"model_name should end with '.pt' or '.pth'\"\n    \n    model_save_path = target_dir_path / model_name\n    print(f\"[INFO] Saving model to: {model_save_path}\")\n    torch.save(obj=model.state_dict(), f=model_save_path)\n\n\ndef load_model(model: nn.Module, target_dir: str, model_name: str):\n    \"\"\"\n    Load the model's state dictionary from a given directory.\n    \"\"\"\n    assert model_name.endswith(\".pth\") or model_name.endswith(\".pt\"), \"model_name should end with '.pt' or '.pth'\"\n    target_dir_path = Path(target_dir)\n    \n    model_load_path = target_dir_path / model_name\n    assert model_load_path.is_file(), f\"Model file not found at: {model_load_path}\"\n    \n    print(f\"[INFO] Loading model from: {model_load_path}\")\n    model.load_state_dict(torch.load(model_load_path))  # Load the model's state dictionary\n    return model","metadata":{"_uuid":"c7e1c8d0-6f0c-499f-88fe-1ea5db6fc3f0","_cell_guid":"8c2a8f57-9c10-4eea-9f21-7d979ff52a32","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-11-05T05:59:07.166704Z","iopub.execute_input":"2024-11-05T05:59:07.166973Z","iopub.status.idle":"2024-11-05T05:59:07.179453Z","shell.execute_reply.started":"2024-11-05T05:59:07.166944Z","shell.execute_reply":"2024-11-05T05:59:07.178496Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def seed_everything(seed=42):\n    \"\"\"\n    Set a seed for reproducibility across various libraries.\n    \"\"\"\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)  # Set the PYTHONHASHSEED environment variable\n    np.random.seed(seed)  # Set the seed for NumPy\n    torch.manual_seed(seed)  # Set the seed for PyTorch\n    torch.cuda.manual_seed(seed)  # Set the seed for CUDA (if using GPU)\n    torch.backends.cudnn.deterministic = True  # Make CuDNN deterministic","metadata":{"_uuid":"dad8dc71-5ddb-4ea2-8613-bf4ba197a23a","_cell_guid":"f3e17218-9f22-45fa-b51d-f706f7bed20a","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-11-05T05:59:07.181778Z","iopub.execute_input":"2024-11-05T05:59:07.182080Z","iopub.status.idle":"2024-11-05T05:59:07.192498Z","shell.execute_reply.started":"2024-11-05T05:59:07.182036Z","shell.execute_reply":"2024-11-05T05:59:07.191674Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Hyperparamters & Directories","metadata":{"_uuid":"87da8df9-15c4-4542-995e-f0efc7783c3d","_cell_guid":"0d5407db-9c2e-45c9-a387-88ebead4a89f","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# Hyperparameters setup\nNUM_EPOCHS = 20\nBATCH_SIZE = 32\nLEARNING_RATE = 1e-4\nNUM_WORKERS = 4\n\nseed_everything(seed=42)\n\n# Setup device (GPU if available, else CPU)\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Device: {device}\")","metadata":{"_uuid":"6a86efd9-86fd-43bb-9776-44bd964ab8a0","_cell_guid":"b8099f1f-dee6-4fec-bdf3-8f845bac207e","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-11-05T05:59:07.193686Z","iopub.execute_input":"2024-11-05T05:59:07.194016Z","iopub.status.idle":"2024-11-05T05:59:07.204032Z","shell.execute_reply.started":"2024-11-05T05:59:07.193985Z","shell.execute_reply":"2024-11-05T05:59:07.203176Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"clip_model, preprocess = clip.load(\"ViT-B/32\", device=device)","metadata":{"_uuid":"cf0a5ed3-d299-45b0-bdf1-87f4febd4edf","_cell_guid":"a801b139-4aa2-4c8c-87b9-a6bb017488ba","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-11-05T05:59:07.205185Z","iopub.execute_input":"2024-11-05T05:59:07.205820Z","iopub.status.idle":"2024-11-05T05:59:11.514820Z","shell.execute_reply.started":"2024-11-05T05:59:07.205778Z","shell.execute_reply":"2024-11-05T05:59:11.513772Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data_dir = \"/kaggle/input/visual-taxonomy\"\ntrain_img_dir = os.path.join(data_dir, \"train_images\")\ntrain_path = os.path.join(data_dir, \"train.csv\")\ntest_img_dir = os.path.join(data_dir, \"test_images\")\ntest_path = os.path.join(data_dir, \"test.csv\")\ncat_path = os.path.join(data_dir, \"category_attributes.parquet\")\n\nprint(f\"{data_dir}\")\nprint(f\"{train_img_dir}\")\nprint(f\"{train_path}\")\nprint(f\"{test_img_dir}\")\nprint(f\"{test_path}\")\nprint(f\"{cat_path}\")","metadata":{"_uuid":"74e686fc-b459-44a2-ba7d-36e8f94737e6","_cell_guid":"31150864-cb2f-418c-be38-b5f5ae84e5ef","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-11-05T05:59:11.516273Z","iopub.execute_input":"2024-11-05T05:59:11.516593Z","iopub.status.idle":"2024-11-05T05:59:11.523873Z","shell.execute_reply.started":"2024-11-05T05:59:11.516560Z","shell.execute_reply":"2024-11-05T05:59:11.522893Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Data","metadata":{"_uuid":"b03718c3-1c68-4b67-b4d8-81bd0dedcb24","_cell_guid":"bea6be69-f5a3-45ae-8353-ab5c9d137541","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# Load datasets\ncategory_attributes = pd.read_parquet(cat_path)\ntrain_df = pd.read_csv(train_path)\ntest_df = pd.read_csv(test_path)","metadata":{"_uuid":"ac4b68b1-f0aa-4e81-a5d1-f9cbb0e42fd0","_cell_guid":"c3efb286-a10e-4f74-a61c-cc4eed8f4131","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-11-05T05:59:11.525089Z","iopub.execute_input":"2024-11-05T05:59:11.525426Z","iopub.status.idle":"2024-11-05T05:59:11.686124Z","shell.execute_reply.started":"2024-11-05T05:59:11.525381Z","shell.execute_reply":"2024-11-05T05:59:11.685121Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create a dictionary mapping category to attributes and the number of attributes\ncategory_to_attributes = {}\nfor _, row in category_attributes.iterrows():\n    category_to_attributes[row['Category']] = {\n        \"num_attributes\": row[\"No_of_attribute\"],\n        \"attributes\": row[\"Attribute_list\"]\n    }\ncategory_to_attributes","metadata":{"_uuid":"8368c224-d9ab-473f-b386-3c4e7da21ec2","_cell_guid":"1d048b26-4455-40eb-a5c3-ae527e2a657f","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-11-05T05:59:11.687422Z","iopub.execute_input":"2024-11-05T05:59:11.687748Z","iopub.status.idle":"2024-11-05T05:59:11.704807Z","shell.execute_reply.started":"2024-11-05T05:59:11.687715Z","shell.execute_reply":"2024-11-05T05:59:11.703958Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load the attribute information from `category_attributes.parquet`\ncategory_attributes_df = pd.read_parquet(cat_path)\n\n# Initialize a dictionary to store vocabularies for each category's attributes\nvocabularies = defaultdict(lambda: defaultdict(dict))\n\n# Parse through each category to build attribute vocabularies\nfor _, row in category_attributes_df.iterrows():\n    category = row[\"Category\"]\n    attributes = row[\"Attribute_list\"]\n    num_attributes = row[\"No_of_attribute\"]\n    \n    # Initialize vocabularies for each attribute in the category\n    for i, attribute_name in enumerate(attributes[:num_attributes]):\n        vocabularies[category][attribute_name] = {}  # Unknown class initialized with 0 ID\n\n# Load training data\ntrain_df = pd.read_csv(train_path)\n\n# Populate vocabularies with unique values for each attribute\nfor _, row in train_df.iterrows():\n    category = row[\"Category\"]\n    for i, attribute_name in enumerate(vocabularies[category].keys()):\n        attr_value = row[f\"attr_{i+1}\"]\n        \n        # Add unique attribute values to the vocabulary\n        if attr_value not in vocabularies[category][attribute_name]:\n            vocabularies[category][attribute_name][attr_value] = len(vocabularies[category][attribute_name])\n\n# Example output for vocabularies\nprint(\"Sample vocabulary for 'Sarees' category attributes:\")\nfor attr_name, attr_vocab in vocabularies[\"Sarees\"].items():\n    print(f\"{attr_name}: {attr_vocab}\")","metadata":{"_uuid":"1626f505-1068-42ed-828e-73cc19cfd6cc","_cell_guid":"5c2f87ff-5c56-4649-afae-0d71ec4b0184","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-11-05T05:59:11.708579Z","iopub.execute_input":"2024-11-05T05:59:11.709158Z","iopub.status.idle":"2024-11-05T05:59:19.205531Z","shell.execute_reply.started":"2024-11-05T05:59:11.709107Z","shell.execute_reply":"2024-11-05T05:59:19.204564Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\n# Define the CategoryAttributes class\nclass CategoryAttributes:\n    def __init__(self, name, num_attributes):\n        self.name = name\n        self.num_attributes = num_attributes\n        self.attributes = {}  # Dictionary to hold attribute names and their values\n\n    def add_attribute(self, attribute_name):\n        if attribute_name not in self.attributes:\n            self.attributes[attribute_name] = {}\n\n    def add_value(self, attribute_name, value):\n        if attribute_name in self.attributes:\n            if value not in self.attributes[attribute_name]:\n                self.attributes[attribute_name][value] = len(self.attributes[attribute_name])\n\ncategory_attributes_df = pd.read_parquet(cat_path)\n\n# Initialize a list to store CategoryAttributes instances\ncategory_attributes_list = []\n\n# Parse through each category to build attribute vocabularies\nfor _, row in category_attributes_df.iterrows():\n    category_name = row[\"Category\"]\n    num_attributes = row[\"No_of_attribute\"]\n    \n    # Create a new CategoryAttributes instance\n    category_attr = CategoryAttributes(category_name, num_attributes)\n    \n    # Initialize attributes for each category\n    attributes = row[\"Attribute_list\"]\n    for attribute_name in attributes[:num_attributes]:\n        category_attr.add_attribute(attribute_name)\n    \n    # Store the CategoryAttributes instance\n    category_attributes_list.append(category_attr)\n\n# Adjust the path accordingly\ntrain_df = pd.read_csv(train_path)\n\n# Populate the attributes with unique values from training data\nfor _, row in train_df.iterrows():\n    category_name = row[\"Category\"]\n    \n    # Find the corresponding CategoryAttributes instance\n    for category_attr in category_attributes_list:\n        if category_attr.name == category_name:\n            for i, attribute_name in enumerate(category_attr.attributes.keys()):\n                attr_value = row[f\"attr_{i+1}\"]\n                \n                # Add unique attribute values to the corresponding attribute\n                category_attr.add_value(attribute_name, attr_value)\n\n# Example output for vocabularies\nfor category_attr in category_attributes_list:\n    print(f\"Sample vocabulary for '{category_attr.name}' category attributes:\")\n    for attr_name, values in category_attr.attributes.items():\n        print(f\"{attr_name}: {values}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Generate prompts for each attribute class in the vocabularies\nattribute_prompts = defaultdict(lambda: defaultdict(list))\ntext_features = defaultdict(lambda: defaultdict(list))\n\nfor category, attributes in vocabularies.items():\n    for attr_name, classes in attributes.items():\n        prompts = [f\"a photo of a {class_name} for the {attr_name} in {category}\" for class_name in classes.keys()]\n        attribute_prompts[category][attr_name] = prompts\n\n        # Tokenize and encode the prompts for each attribute\n        tokens = clip.tokenize(prompts).to(device)\n        with torch.no_grad():\n            encoded_text = clip_model.encode_text(tokens)\n            text_features[category][attr_name] = encoded_text","metadata":{"_uuid":"22c1abee-329c-4a0f-8282-bb6d9d2b3189","_cell_guid":"8f863d61-fb5c-4e06-b90b-9fd4bc95164c","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-11-05T05:59:19.206815Z","iopub.execute_input":"2024-11-05T05:59:19.207222Z","iopub.status.idle":"2024-11-05T05:59:19.684584Z","shell.execute_reply.started":"2024-11-05T05:59:19.207177Z","shell.execute_reply":"2024-11-05T05:59:19.683779Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for category, attributes in vocabularies.items():\n    print(f\"Category: {category}\")\n    for attr_name, class_dict in attributes.items():\n        print(f\"  Attribute: {attr_name} - Classes: {list(class_dict.keys())}\")","metadata":{"_uuid":"2ef05aa5-74ed-49ab-bebb-b4b7b5ae6213","_cell_guid":"10e79d06-f5bf-4f45-8c0d-548d4813f720","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-11-05T06:01:43.871746Z","iopub.execute_input":"2024-11-05T06:01:43.872505Z","iopub.status.idle":"2024-11-05T06:01:43.878635Z","shell.execute_reply.started":"2024-11-05T06:01:43.872462Z","shell.execute_reply":"2024-11-05T06:01:43.877700Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Data augmentation and normalization for training\ndata_transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.RandomHorizontalFlip(p=0.5),\n    transforms.RandomVerticalFlip(p=0.3),\n    transforms.RandomRotation(degrees=15),\n    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n    transforms.RandomPerspective(distortion_scale=0.2, p=0.5),\n    transforms.ToTensor(),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n    transforms.RandomErasing(p=0.2, scale=(0.02, 0.33), ratio=(0.3, 3.3), value='random'),\n    transforms.RandomApply([transforms.GaussianBlur(kernel_size=3)], p=0.3)\n])","metadata":{"_uuid":"e79322e7-ffb2-45b8-84c3-9d9787d07671","_cell_guid":"28ff07dc-6d05-4f75-b4dd-f9217f62e68c","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-11-05T05:59:19.685731Z","iopub.execute_input":"2024-11-05T05:59:19.686051Z","iopub.status.idle":"2024-11-05T05:59:19.694095Z","shell.execute_reply.started":"2024-11-05T05:59:19.686017Z","shell.execute_reply":"2024-11-05T05:59:19.693195Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset\n\nclass FashionDataset(Dataset):\n    def __init__(self, dataframe, image_dir, vocabularies, transform=None):\n        self.data = dataframe\n        self.image_dir = image_dir\n        self.vocabularies = vocabularies\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        row = self.data.iloc[idx]\n        img_id = row[\"id\"]\n        category = row[\"Category\"]\n        \n        # Load image\n        image_path = os.path.join(data_dir, f\"{self.image_dir}/{img_id:06d}.jpg\")\n        image = Image.open(image_path).convert(\"RGB\")\n        if self.transform:\n            image = self.transform(image)\n\n        # Get attribute labels as IDs\n        attributes = []\n        for i, attr_name in enumerate(self.vocabularies[category].keys()):\n            attr_value = row[f\"attr_{i+1}\"]\n            attr_id = self.vocabularies[category][attr_name].get(attr_value, 0)  # 0 for unknown\n            attributes.append(attr_id)\n\n        return image, category, torch.tensor(attributes)","metadata":{"_uuid":"8fb3f921-1a58-4f0a-8fdf-ae611349eee5","_cell_guid":"fbcbe3cd-4e3d-4a76-9179-32c3fdee1a64","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-11-05T05:59:19.695230Z","iopub.execute_input":"2024-11-05T05:59:19.695584Z","iopub.status.idle":"2024-11-05T05:59:19.705192Z","shell.execute_reply.started":"2024-11-05T05:59:19.695535Z","shell.execute_reply":"2024-11-05T05:59:19.704364Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Model","metadata":{"_uuid":"cf9ae954-b5b5-4963-9f41-143ebd40c398","_cell_guid":"e873b3ba-8609-49d2-ad68-af50d6150788","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"import torch.nn as nn\n\nclass MultiOutputCLIPModel(nn.Module):\n    def __init__(self, clip_model, category_to_attributes, vocabularies):\n        super(MultiOutputCLIPModel, self).__init__()\n        self.clip_model = clip_model\n        self.vocabularies = vocabularies\n\n        # Initialize classifiers for each category's attributes\n        self.classifiers = nn.ModuleDict()\n        for category, attributes in vocabularies.items():\n            classifiers = nn.ModuleList()\n            for attr_name in attributes:\n                num_classes = len(attributes[attr_name])  # Size of vocabulary for each attribute\n                classifier = nn.Linear(clip_model.visual.output_dim, num_classes)\n                classifiers.append(classifier)\n            self.classifiers[category] = classifiers\n\n    def forward(self, images, categories):\n        with torch.no_grad():\n            image_features = self.clip_model.encode_image(images)\n\n        outputs = {}\n        for i, category in enumerate(categories):\n            classifiers = self.classifiers[category]\n            outputs[category] = [classifier(image_features[i]) for classifier in classifiers]\n\n        return outputs","metadata":{"_uuid":"8b0d8064-af4f-462f-90d0-297654bbb62d","_cell_guid":"2e5d69d6-0f03-4a36-9287-9b3fb4206cfe","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-11-05T05:59:19.706164Z","iopub.execute_input":"2024-11-05T05:59:19.706476Z","iopub.status.idle":"2024-11-05T05:59:19.719989Z","shell.execute_reply.started":"2024-11-05T05:59:19.706445Z","shell.execute_reply":"2024-11-05T05:59:19.719191Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Training","metadata":{"_uuid":"284308a0-4b92-495e-8121-2094366696ca","_cell_guid":"d9e938b2-5360-49ad-a264-20f999c0db42","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# # Prepare dataloaders\n# train_dataset = FashionDataset(train_df, 'train_images', category_to_attributes, preprocess)\n# train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n\n# # Initialize model, optimizer, and loss function\n# multi_output_model = MultiOutputCLIPModel(clip_model, category_to_attributes, attribute_prompts).to(device)\n# optimizer = torch.optim.Adam(multi_output_model.parameters(), lr=1e-4)\n# criterion = nn.CrossEntropyLoss()  # Use Cross-Entropy as each attribute is mutually exclusive\n\n# # Training loop\n# for epoch in range(NUM_EPOCHS):\n#     multi_output_model.train()\n#     total_loss = 0\n    \n#     with tqdm(train_loader, unit=\"batch\") as tepoch:\n#         tepoch.set_description(f\"Epoch {epoch + 1}/{NUM_EPOCHS}\")\n\n#         for images, categories, attributes in tepoch:\n#             images = images.to(device)\n            \n#             # Forward pass\n#             outputs = multi_output_model(images, categories)\n#             loss = 0\n\n#             # Calculate loss for each category and attribute\n#             for i, category in enumerate(categories):\n#                 for j, attribute in enumerate(vocabularies[category].keys()):\n#                     attr_id = attributes[i][j]\n#                     output = outputs[category][j].unsqueeze(0)  # shape to [1, num_classes] if necessary\n                    \n#                     # Calculate loss for each attribute\n#                     loss += criterion(output, attr_id.view(-1))\n\n#             # Backpropagation\n#             optimizer.zero_grad()\n#             loss.backward()\n#             optimizer.step()\n\n#             # Update the total loss\n#             total_loss += loss.item()\n#             tepoch.set_postfix(loss=total_loss / len(train_loader))\n\n#     print(f\"Epoch {epoch + 1}, Average Loss: {total_loss / len(train_loader)}\")","metadata":{"_uuid":"e063cf23-5689-49d4-9cae-940c5b3a80db","_cell_guid":"5dfcc882-7e52-4df6-8509-77f5ab2046d5","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-11-05T05:59:20.183563Z","iopub.execute_input":"2024-11-05T05:59:20.183858Z","iopub.status.idle":"2024-11-05T05:59:20.192012Z","shell.execute_reply.started":"2024-11-05T05:59:20.183827Z","shell.execute_reply":"2024-11-05T05:59:20.191184Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def zero_shot_predict(id, category, text_features, attribute_prompts):\n    # Generate the image path and load/preprocess the image\n    image_path = os.path.join(train_img_dir, f\"{id:06d}.jpg\")\n    image = preprocess(Image.open(image_path)).unsqueeze(0).to(device)\n    \n    # Encode image with CLIP model\n    with torch.no_grad():\n        image_features = clip_model.encode_image(image)\n\n    predictions = {}\n    \n    # Iterate over each attribute and its associated class features\n    for attr_name, class_features in text_features[category].items():\n        # Compute cosine similarity between image and each class feature vector\n        similarities = (image_features @ class_features.T).squeeze(0)\n        best_class_idx = similarities.argmax().item()\n        \n        # Extract the prompt and split to get the predicted class name\n        best_prompt = attribute_prompts[category][attr_name][best_class_idx]\n        best_class_name = best_prompt.split()[-1]  # Assuming class name is the last word\n\n        # Debug: Print the prompt and class name to verify correctness\n        print(f\"Attribute: {attr_name}, Prompt: {best_prompt}, Predicted Class: {best_class_name}\")\n        \n        # Store the prediction\n        predictions[attr_name] = best_class_name\n\n    return predictions","metadata":{"_uuid":"832527f5-278d-43c8-888e-5e896c5b2e6b","_cell_guid":"7cca642b-9297-4d45-a41e-c68b6e30091f","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-11-05T05:59:20.193362Z","iopub.execute_input":"2024-11-05T05:59:20.194158Z","iopub.status.idle":"2024-11-05T05:59:20.204194Z","shell.execute_reply.started":"2024-11-05T05:59:20.194099Z","shell.execute_reply":"2024-11-05T05:59:20.203167Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Initialize an empty list to store results\nresults = []\nmax_examples = 20  # Limit the number of examples for testing\n\n# Iterate over each row in train_df to generate predictions\nfor idx, row in train_df.iterrows():\n    if idx >= max_examples:\n        break  # Stop after max_examples for testing purposes\n    \n    # Get image ID and category for the current row\n    image_id = row[\"id\"]\n    category = row[\"Category\"]\n    \n    # Perform zero-shot prediction with debugging information\n    predictions = zero_shot_predict(image_id, category, text_features, attribute_prompts)\n    \n    # Build the row dictionary in the required format\n    row_result = {\n        \"id\": image_id,\n        \"Category\": category\n    }\n    \n    # Fill in each attribute prediction based on `vocabularies` keys\n    for i, attr_name in enumerate(vocabularies[category].keys()):\n        row_result[f\"attr_{i+1}\"] = predictions.get(attr_name, \"Unknown\")  # Set to \"Unknown\" if no prediction\n    \n    # Append the result to the results list\n    results.append(row_result)\n\n# Convert the list of results into a DataFrame\nresults_df = pd.DataFrame(results)\n\n# Display the first few rows of the predictions DataFrame\nprint(results_df.head())","metadata":{"_uuid":"7585555d-05da-40b9-a250-c628b632f8ac","_cell_guid":"a2f86f31-96fd-48b1-ad64-e1b7fdcffe0b","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-11-05T05:59:20.205526Z","iopub.execute_input":"2024-11-05T05:59:20.205819Z","iopub.status.idle":"2024-11-05T05:59:20.542074Z","shell.execute_reply.started":"2024-11-05T05:59:20.205788Z","shell.execute_reply":"2024-11-05T05:59:20.540929Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Generate prompts with modified structure for attribute classes\nfor category, attributes in vocabularies.items():\n    for attr_name, classes in attributes.items():\n        # Revised prompt structure with class name at the start\n        prompts = [f\"a photo of a {class_name} for the {attr_name} in {category}\" for class_name in classes.keys()]\n        attribute_prompts[category][attr_name] = prompts\n\n        # Tokenize and encode the prompts\n        tokens = clip.tokenize(prompts).to(device)\n        with torch.no_grad():\n            encoded_text = clip_model.encode_text(tokens)\n            text_features[category][attr_name] = encoded_text\n\n# Revised zero-shot prediction function with debug info\ndef zero_shot_predict(id, category, text_features, attribute_prompts):\n    # Load and preprocess image\n    image_path = os.path.join(train_img_dir, f\"{id:06d}.jpg\")\n    image = preprocess(Image.open(image_path)).unsqueeze(0).to(device)\n    \n    # Encode image\n    with torch.no_grad():\n        image_features = clip_model.encode_image(image)\n\n    predictions = {}\n    \n    # Iterate over attributes to predict each one\n    for attr_name, class_features in text_features[category].items():\n        similarities = (image_features @ class_features.T).squeeze(0)\n        best_class_idx = similarities.argmax().item()\n        \n        # Extract the class name based on updated prompt structure\n        best_prompt = attribute_prompts[category][attr_name][best_class_idx]\n        best_class_name = best_prompt.split()[3]  # Extract based on revised prompt format\n        \n        # Debug print to confirm\n        print(f\"Attribute: {attr_name}, Prompt: {best_prompt}, Predicted Class: {best_class_name}\")\n        \n        # Save prediction\n        predictions[attr_name] = best_class_name\n\n    return predictions\n\n# Run a loop with debug checks\nresults = []\nfor idx, row in train_df.iterrows():\n    if idx >= 20:\n        break  # Limit examples for testing\n    \n    # Get predictions\n    image_id = row[\"id\"]\n    category = row[\"Category\"]\n    predictions = zero_shot_predict(image_id, category, text_features, attribute_prompts)\n    \n    # Format row results\n    row_result = {\n        \"id\": image_id,\n        \"Category\": category\n    }\n    for i, attr_name in enumerate(vocabularies[category].keys()):\n        row_result[f\"attr_{i+1}\"] = predictions.get(attr_name, \"Unknown\")\n\n    results.append(row_result)\n\n# Convert to DataFrame for analysis\nresults_df = pd.DataFrame(results)\nprint(results_df.head())","metadata":{"_uuid":"26e4ae8e-f40a-40b1-a9f8-82e50c03e7ea","_cell_guid":"af2b0874-791c-433c-9fc4-7fba3f9ba1d0","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-11-05T06:02:52.521693Z","iopub.execute_input":"2024-11-05T06:02:52.522413Z","iopub.status.idle":"2024-11-05T06:02:53.321369Z","shell.execute_reply.started":"2024-11-05T06:02:52.522371Z","shell.execute_reply":"2024-11-05T06:02:53.320345Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Metrics, Loss, Optimizer","metadata":{"_uuid":"dfe4edb8-7918-4fd3-9f99-a1c33cb88a25","_cell_guid":"271498f4-30ba-4f2b-afa7-65efc2f6ef5f","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# Example: assume there are 5 categories with [3, 4, 2, 5, 3] attributes respectively\ncategory_attribute_counts = [3, 4, 2, 5, 3]\nmetric = AttributeScoreMetric(category_attribute_counts)\n\n# Assume preds and labels are torch tensors of shape (batch_size, total_attributes)\n# Example: batch size of 10, total attributes = 3 + 4 + 2 + 5 + 3 = 17\nbatch_size = 10\ntotal_attributes = sum(category_attribute_counts)\npreds = torch.randint(0, 2, (batch_size, total_attributes))  # Random predictions for binary classification\nlabels = torch.randint(0, 2, (batch_size, total_attributes))  # Random ground truth labels\n\n# Compute the score\nscore = metric(preds, labels)\nprint(\"Overall Score:\", score.item())","metadata":{"_uuid":"9a0b4da0-7c70-4741-ada6-8f3a87e9dc12","_cell_guid":"5088e191-892b-40e4-bd4a-5e96c2a0fbe1","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-11-05T05:59:20.543319Z","iopub.execute_input":"2024-11-05T05:59:20.544174Z","iopub.status.idle":"2024-11-05T05:59:20.598399Z","shell.execute_reply.started":"2024-11-05T05:59:20.544118Z","shell.execute_reply":"2024-11-05T05:59:20.597182Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Inference","metadata":{"_uuid":"429b427c-0c67-436d-88f4-7e48e8c4586b","_cell_guid":"7c39d1ef-5e10-4905-aa39-514f778f1f15","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"","metadata":{"_uuid":"e132e124-0b7d-4158-b9c9-4fd13f2c7df1","_cell_guid":"a350ae6e-c740-4a86-b09a-fe1f1c7be53c","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Submission","metadata":{"_uuid":"c7ac3bb3-4a86-433c-ade4-e7a13d70079b","_cell_guid":"ddcfa049-2c69-4709-ba47-1f95c48a8a6e","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"","metadata":{"_uuid":"f34e6988-fb85-422e-80b0-b401d5fb0199","_cell_guid":"cdcd4085-4d6d-43e9-b0c4-cd65fbc004b1","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null}]}